apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "nomad.fullname" . }}-worker
  labels:
    app.kubernetes.io/name: {{ include "nomad.name" . }}-worker
    helm.sh/chart: {{ include "nomad.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: {{ .Values.worker.replicas }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "nomad.name" . }}-worker
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "nomad.name" . }}-worker
        app.kubernetes.io/instance: {{ .Release.Name }}
      {{ if .Values.roll }}
      annotations:
        rollme: {{ randAlphaNum 5 | quote }}
      {{ end }}
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: {{ include "nomad.name" . }}-worker
                app.kubernetes.io/instance: {{ .Release.Name }}
      containers:
      - name: {{ include "nomad.name" . }}-worker
        image: "{{ .Values.image.name }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        resources:
          limits:
            memory: "{{ .Values.worker.memlimit }}Gi"
          requests:
            memory: "{{ .Values.worker.memrequest }}Gi"
        volumeMounts:
        - mountPath: /app/nomad.yaml
          name: nomad-conf
          subPath: nomad.yaml
        - mountPath: /app/.volumes/fs/public
          name: public-volume
        - mountPath: /app/.volumes/fs/staging
          name: staging-volume
        - mountPath: /nomad
          name: nomad-volume
        env:
        - name: NOMAD_META_SERVICE
          value: "worker"
        - name: NOMAD_CONSOLE_LOG_LEVEL
          value: "{{ .Values.worker.console_loglevel }}"
        - name: NOMAD_LOGSTASH_LEVEL
          value: "{{ .Values.worker.logstash_loglevel }}"
        - name: NOMAD_CELERY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        {{ if .Values.api.apiSecret }}
        - name: NOMAD_SERVICES_API_SECRET
          valueFrom:
            secretKeyRef:
              name: {{ .Values.api.apiSecret}}
              key: password
        {{ end }}
        {{ if .Values.keycloak.clientSecret }}
        - name: NOMAD_KEYCLOAK_CLIENT_SECRET
          valueFrom:
            secretKeyRef:
              name: {{ .Values.keycloak.clientSecret }}
              key: password
        {{ end }}
        {{ if .Values.keycloak.passwordSecret }}
        - name: NOMAD_KEYCLOAK_PASSWORD
          valueFrom:
            secretKeyRef:
              name: {{ .Values.keycloak.passwordSecret }}
              key: password
        {{ end }}
        command: ["python", "-m", "celery", "-A", "nomad.processing", "worker", "-n", "$(NOMAD_CELERY_NODE_NAME)" {{ if .Values.worker.processes }}, "-c", "{{ .Values.worker.processes }}"{{ end }}{{ if .Values.worker.maxTasksPerChild }}, "--max-tasks-per-child", "{{ .Values.worker.maxTasksPerChild }}"{{ end }}]
        livenessProbe:
          exec:
            command:
            - bash
            - -c
            - NOMAD_LOGSTASH_LEVEL=WARNING python -m celery -A nomad.processing status | grep "$(NOMAD_CELERY_NODE_NAME):.*OK"
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          exec:
            command:
            - bash
            - -c
            - NOMAD_LOGSTASH_LEVEL=WARNING python -m celery -A nomad.processing status | grep "${NOMAD_CELERY_NODE_NAME}:.*OK"
          initialDelaySeconds: 15
          periodSeconds: 30
      nodeSelector:
        nomadtype: {{ .Values.worker.nomadNodeType }}
      imagePullSecrets:
      - name: {{ .Values.image.secret }}
      volumes:
      - name: nomad-conf
        configMap:
          name: {{ include "nomad.fullname" . }}-configmap
      - name: public-volume
        hostPath:
          path: {{ .Values.volumes.public }}
          type: Directory
      - name: staging-volume
        {{ if (eq .Values.worker.storage "memory") }}
        emptyDir:
          medium: 'Memory'
        {{ else }}
        hostPath:
          path: {{ .Values.volumes.staging}}
          type: Directory
        {{ end }}
      - name: nomad-volume
        hostPath:
          path: {{ .Values.volumes.nomad }}
          type: Directory
