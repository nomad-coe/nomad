## Default values for nomad@FAIRDI
version:
  label: "1.1.5"
  isTest: false
  isBeta: false
  usesBetaData: false
  officialUrl: "https://nomad-lab.eu/prod/rae/gui"

meta:
  service: "app"
  homepage: "https://nomad-lab.eu"
  source_url: "https://gitlab.mpcdf.mpg.de/nomad-lab/nomad-FAIR"
  maintainer_email: "markus.scheidgen@physik.hu-berlin.de"

## Everything concerning the container images to be used
image:
  ## The kubernetes docker-registry secret that can be used to access the registry
  #  with the container image in it.
  #  It can be created via:
  #    kubectl create secret docker-registry gitlab-mpcdf --docker-server=gitlab-registry.mpcdf.mpg.de --docker-username=<your-user-name > --docker-password=<yourpass> --docker-email=<email>
  secret: gitlab-mpcdf

  ## The docker container image name without tag
  name: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-fair
  ## The docker container image tag
  tag: latest
  pullPolicy: IfNotPresent

## Ingress can be unable to provide gui and api access through kubernetes ingress (only k8s 1.18+)
ingress:
  enabled: false
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: "32g"
    nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "10"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
  hosts:
    - ""

## Everything concerning the nomad app
app:
  replicas: 1
  ## Number of uvicorn worker.
  worker: 4
  console_loglevel: INFO
  logstash_loglevel: INFO
  nomadNodeType: "public"

## Everything concerning the nomad api
api:
  ## Secret used as cryptographic seed
  secret: "defaultApiSecret"
  ## Limit of unpublished uploads per user, except admin user
  uploadLimit: 10

## Everything concerning the nomad worker
worker:
  replicas: 1
  # request and limit in GB, good prod sizes are 64, 420
  memrequest: 8
  memlimit: 32
  maxTasksPerChild: 128
  console_loglevel: ERROR
  logstash_loglevel: INFO
  ## There are two routing modes "queue" and "worker". The "queue" routing will use a general
  # task queue and spread calc processing task over worker instances. The "worker" routing
  # will send all tasks related to an upload to the same worker
  routing: "queue"
  storage: "disk"
  nomadNodeType: "worker"
  timeout: 7200
  acks_late: false

## Everthing concerning the nomad gui
gui:
  replicas: 1
  ## This variable is used in the GUI to show or hide additional information
  debug: false
  ## automatically gz based on header
  gzip: true
  ## configuration for the interface, menus, options, etc.
  config:
    search_contexts:
      options:
        entries:
          columns:
            enable:
              - results.material.chemical_formula_hill
              - results.method.method_name
              - upload_create_time
              - authors

encyclopedia:
  ## enable links to the 'new' encyclopedia
  enabled: true

aitoolkit:
  ## enable aitoolkit references
  enabled: false

## Everything concerning the nginx that serves the gui, proxies the api
#  It is run via NodePort service
proxy:
  # Set a nodePort to create a NodePort service instead of ClusterIP. Also set a nodeIP for the externalIP.
  nodePort:
  nodeIP:
  timeout: 120
  editTimeout: 1800
  external:
    host: "nomad-lab.eu"
    port: 80
    path: "/fairdi/nomad/latest"
    https: true

## configuration of the chart dependency for rabbitmq
rabbitmq:
  persistence:
    enabled: false
  nodeSelector:
    nomadtype: public
  image.pullSecrets: nil
  auth:
    username: rabbitmq
    password: rabbitmq
    erlangCookie: SWQOKODSQALRPCLNMEQG

## A common name/prefix for all dbs and indices.
dbname: fairdi_nomad_latest

## Databases that are not run within the cluster.
#  To run databases in the cluster, use the nomad-full helm chart.

mongo:
  host: nomad-flink-01.esc
  port: 27017

elastic:
  host: nomad-flink-01.esc
  port: 9200
  timeout: 60
  bulkTimeout: 600
  bulkSize: 1000
  entriesPerMaterialCap: 1000

logstash:
  enabled: true
  port: 5000
  host: nomad-flink-01.esc

kibana:
  port: 5601
  host: nomad-flink-01.esc

mail:
  enabled: false
  host: "localhost"
  port: 25
  from: "support@nomad-lab.eu"

client:
  username: admin

keycloak:
  serverExternalUrl: "https://nomad-lab.eu/fairdi/keycloak/auth/"
  serverUrl: "https://nomad-lab.eu/keycloak/auth/"
  realmName: "fairdi_nomad_test"
  username: "admin"
  clientId: "nomad_public"
  guiClientId: "nomad_public"
  admin_user_id: "00000000-0000-0000-0000-000000000000"

## Everything concerning the data that is used by the service
volumes:
  prefixSize: 1
  public: /nomad/fairdi/latest/fs/public
  staging: /nomad/fairdi/latest/fs/staging
  tmp: /nomad/fairdi/latest/fs/tmp
  nomad: /nomad

springerDbPath: /nomad/fairdi/db/data/springer.msg

reprocess:
  rematchPublished: true
  reprocessExistingEntries: true
  useOriginalParser: false
  addMatchedEntriesToPublished: false
  deleteUnmatchedPublishedEntries: false
  indexIndividualEntries: false

process:
  reuseParser: true
  indexMaterials: true

datacite:
  enabled: false
  prefix: "10.17172"

services:
  jupyterhub:
    enabled: false

jupyterhub:
  # fullnameOverride: null
  # nameOverride: "north"
  proxy:
    service:
      type: ClusterIP
  hub:
    allowNamedServers: true
    shutdownOnLogout: true
    config:
      JupyterHub:
        authenticator_class: generic-oauth
      Authenticator:
        auto_login: true
        enable_auth_state: true
      GenericOAuthenticator:
        client_id: nomad_public
        oauth_callback_url:
        authorize_url: https://nomad-lab.eu/fairdi/keycloak/auth/realms/fairdi_nomad_test/protocol/openid-connect/auth
        token_url: https://nomad-lab.eu/fairdi/keycloak/auth/realms/fairdi_nomad_test/protocol/openid-connect/token
        userdata_url: https://nomad-lab.eu/fairdi/keycloak/auth/realms/fairdi_nomad_test/protocol/openid-connect/userinfo
        login_service: keycloak
        username_key: preferred_username
        userdata_params:
          state: state
  cull:
    enabled: true
    timeout: 3600
    every: 600
    removeNamedServers: true
  prePuller:
    hook:
      enabled: true
    continuous:
      enabled: false
  scheduling:
    userScheduler:
      enabled: false
    podPriority:
      enabled: false
    userPlaceholder:
      enabled: false
      replicas: 0
