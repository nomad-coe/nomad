apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "nomad.fullname" . }}-worker
  labels:
    app.kubernetes.io/name: {{ include "nomad.name" . }}-worker
    helm.sh/chart: {{ include "nomad.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: {{ .Values.worker.replicas }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "nomad.name" . }}-worker
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "nomad.name" . }}-worker
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      containers:
      - name: {{ include "nomad.name" . }}-worker
        image: "{{ .Values.images.nomad.name }}:{{ .Values.images.nomad.tag }}"
        resources:
          limits:
            memory: "{{ .Values.worker.memlimit }}Gi"
          requests:
            memory: "{{ .Values.worker.memrequest }}Gi"
        volumeMounts:
        - mountPath: /app/.volumes/fs/public
          name: public-volume
        - mountPath: /app/.volumes/fs/staging
          name: staging-volume
        - mountPath: /nomad
          name: nomad-volume
        env:
        - name: NOMAD_FILES_TMP_DIR
          value: "{{ .Values.volumes.tmp }}"
        - name: NOMAD_SERVICE
          value: "worker"
        - name: NOMAD_RELEASE
          value: "{{ .Release.Name }}"
        - name: NOMAD_LOGSTASH_HOST
          value: "{{ .Values.logstash.host }}"
        - name: NOMAD_LOGSTASH_TCPPORT
          value: "{{ .Values.logstash.port }}"
        - name: NOMAD_CONSOLE_LOGLEVEL
          value: "{{ .Values.worker.console_loglevel }}"
        - name: NOMAD_LOGSTASH_LEVEL
          value: "{{ .Values.worker.logstash_loglevel }}"
        - name: NOMAD_RABBITMQ_HOST
          value: "{{ .Release.Name }}-rabbitmq"
        - name: NOMAD_ELASTIC_HOST
          value: "{{ .Values.elastic.host }}"
        - name: NOMAD_ELASTIC_PORT
          value: "{{ .Values.elastic.port }}"
        - name: NOMAD_ELASTIC_INDEX_NAME
          value: "{{ .Values.dbname }}"
        - name: NOMAD_MONGO_HOST
          value: "{{ .Values.mongo.host }}"
        - name: NOMAD_MONGO_PORT
          value: "{{ .Values.mongo.port }}"
        - name: NOMAD_MONGO_DB_NAME
          value: "{{ .Values.dbname }}"
        - name: NOMAD_COE_REPO_DB_HOST
          value: "{{ .Values.postgres.host }}"
        - name: NOMAD_COE_REPO_DB_PORT
          value: "{{ .Values.postgres.port }}"
        - name: NOMAD_COE_REPO_DB_NAME
          value: "{{ .Values.dbname }}"
        - name: NOMAD_UPLOAD_URL
          value: "{{ .Values.uploadurl }}"
        - name: NOMAD_SMTP_HOST
          value: "{{ .Values.mail.host }}"
        - name: NOMAD_SMTP_PORT
          value: "{{ .Values.mail.port }}"
        - name: NOMAD_SMTP_USER
          value: "{{ .Values.mail.user }}"
        - name: NOMAD_SMTP_PASSWORD
          value: "{{ .Values.mail.password }}"
        - name: NOMAD_MAIL_FROM
          value: "{{ .Values.mail.from }}"
        - name: NOMAD_CONSOLE_LOGLEVEL
          value: "ERROR"
        - name: NOMAD_CELERY_ROUTING
          value: "{{ .Values.worker.routing }}"
        - name: NOMAD_FILES_PREFIX_SIZE
          value: "{{ .Values.volumes.prefixSize }}"
        command: ["python", "-m", "celery", "worker", "-A", "nomad.processing", "-Q", "celery,calcs,uploads"]
        livenessProbe:
          exec:
            command:
            - bash
            - -c
            - NOMAD_LOGSTASH_LEVEL=30 python -m celery -A nomad.processing status | grep "${HOSTNAME}:.*OK"
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          exec:
            command:
            - bash
            - -c
            - NOMAD_LOGSTASH_LEVEL=30 python -m celery -A nomad.processing status | grep "${HOSTNAME}:.*OK"
          initialDelaySeconds: 5
          periodSeconds: 120
      nodeSelector:
        nomadtype: worker
      imagePullSecrets:
      - name: {{ .Values.images.secret }}
      imagePullPolicy: always
      volumes:
      - name: public-volume
        hostPath:
          path: {{ .Values.volumes.public }}
          type: Directory
      - name: staging-volume
        {{ if (eq .Values.worker.routing "worker") }}
        emptyDir:
          medium: 'Memory'
        {{ else }}
        hostPath:
          path: {{ .Values.volumes.staging}}
          type: Directory
        {{ end }}
      - name: nomad-volume
        hostPath:
          path: {{ .Values.volumes.nomad }}
          type: Directory
